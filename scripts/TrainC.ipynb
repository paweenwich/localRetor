{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "from dgl.data.utils import Subset\n",
    "from dgllife.utils import WeaveAtomFeaturizer, CanonicalBondFeaturizer, smiles_to_bigraph, EarlyStopping\n",
    "\n",
    "from models import LocalRetro\n",
    "\n",
    "from utils import init_featurizer, mkdir_p, get_configure, load_model, load_dataloader, predict\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        smiles, bg, atom_labels, bond_labels = batch_data\n",
    "        if len(smiles) == 1:\n",
    "            continue\n",
    "           \n",
    "        atom_labels, bond_labels = atom_labels.to(args['device']), bond_labels.to(args['device'])\n",
    "        atom_logits, bond_logits, _ = predict(args, model, bg)\n",
    "\n",
    "        loss_a = loss_criterion(atom_logits, atom_labels)\n",
    "        loss_b = loss_criterion(bond_logits, bond_labels)\n",
    "        total_loss = torch.cat([loss_a, loss_b]).mean()\n",
    "        train_loss += total_loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()      \n",
    "        total_loss.backward() \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), args['max_clip'])\n",
    "        optimizer.step()\n",
    "                \n",
    "        if batch_id % args['print_every'] == 0:\n",
    "            print('\\repoch %d/%d, batch %d/%d, loss %.4f' % (epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), total_loss), end='', flush=True)\n",
    "\n",
    "    print('\\nepoch %d/%d, training loss: %.4f' % (epoch + 1, args['num_epochs'], train_loss/batch_id))\n",
    "\n",
    "def run_an_eval_epoch(args, model, data_loader, loss_criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            smiles, bg, atom_labels, bond_labels = batch_data\n",
    "            atom_labels, bond_labels = atom_labels.to(args['device']), bond_labels.to(args['device'])\n",
    "            atom_logits, bond_logits, _ = predict(args, model, bg)\n",
    "            \n",
    "            loss_a = loss_criterion(atom_logits, atom_labels)\n",
    "            loss_b = loss_criterion(bond_logits, bond_labels)\n",
    "            total_loss = torch.cat([loss_a, loss_b]).mean()\n",
    "            val_loss += total_loss.item()\n",
    "    return val_loss/batch_id\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    #KKK\n",
    "    model_path = '../modelsC'\n",
    "    model_name = 'LocalRetro_%s.pth' % args['dataset']\n",
    "    args['model_path'] = model_path +'/' + model_name\n",
    "    args['config_path'] = '../data/configs/%s' % args['config']\n",
    "    args['data_dir'] = '../data/%s' % args['dataset']\n",
    "    mkdir_p(model_path)                          \n",
    "    args = init_featurizer(args)\n",
    "    model, loss_criterion, optimizer, scheduler, stopper = load_model(args)   \n",
    "    train_loader, val_loader, test_loader = load_dataloader(args)\n",
    "    for epoch in range(args['num_epochs']):\n",
    "        print(\"Running\",epoch)\n",
    "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "        val_loss = run_an_eval_epoch(args, model, val_loader, loss_criterion)\n",
    "        early_stop = stopper.step(val_loss, model) \n",
    "        scheduler.step()\n",
    "        print('epoch %d/%d, validation loss: %.4f' %  (epoch + 1, args['num_epochs'], val_loss))\n",
    "        print('epoch %d/%d, Best loss: %.4f' % (epoch + 1, args['num_epochs'], stopper.best_score))\n",
    "        if early_stop:\n",
    "            print ('Early stopped!!')\n",
    "            break\n",
    "\n",
    "    stopper.load_checkpoint(model)\n",
    "    test_loss = run_an_eval_epoch(args, model, test_loader, loss_criterion)\n",
    "    print('test loss: %.4f' % test_loss)\n",
    "\n",
    "import tqdm as notebook_tqdm\n",
    "args = {'gpu': 'cuda:0', 'dataset': 'USPTO_50K', 'config': 'default_config.json', 'batch_size': 1, 'num_epochs': 50, 'patience': 5, 'max_clip': 20, 'learning_rate': 0.0001, 'weight_decay': 1e-06, 'schedule_step': 10, 'num_workers': 0, 'print_every': 20, 'mode': 'train'}\n",
    "args['device'] = torch.device(args['gpu']) if torch.cuda.is_available() else torch.device('cpu')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../modelsC already exists.\n",
      "node_out_feats 320\n",
      "Parameters of loaded LocalRetro:\n",
      "{'attention_heads': 8, 'attention_layers': 1, 'batch_size': 16, 'edge_hidden_feats': 64, 'node_out_feats': 320, 'num_step_message_passing': 6, 'AtomTemplate_n': 124, 'BondTemplate_n': 548, 'in_node_feats': 80, 'in_edge_feats': 13}\n"
     ]
    }
   ],
   "source": [
    "from dgllife.model import MPNNGNN\n",
    "from model_utils import pair_atom_feats, unbatch_mask, unbatch_feats, Global_Reactivity_Attention\n",
    "def pair_atom_feats(g, node_feats):\n",
    "    sg = g.remove_self_loop() # in case g includes self-loop\n",
    "    atom_pair_list = torch.transpose(sg.adjacency_matrix().coalesce().indices(), 0, 1)\n",
    "    atom_pair_idx1 = atom_pair_list[:,0]\n",
    "    atom_pair_idx2 = atom_pair_list[:,1]\n",
    "    atom_pair_feats = torch.cat((node_feats[atom_pair_idx1], node_feats[atom_pair_idx2]), dim = 1)\n",
    "    print(\"pair_atom_feats\",atom_pair_feats)\n",
    "    return atom_pair_feats\n",
    "\n",
    "class LocalRetroC(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_in_feats,\n",
    "                 edge_in_feats,\n",
    "                 node_out_feats,\n",
    "                 edge_hidden_feats,\n",
    "                 num_step_message_passing,\n",
    "                 attention_heads,\n",
    "                 attention_layers,\n",
    "                 AtomTemplate_n, \n",
    "                 BondTemplate_n):\n",
    "        super(LocalRetroC, self).__init__()\n",
    "        print(\"node_out_feats\",node_out_feats)                \n",
    "        self.mpnn = MPNNGNN(node_in_feats=node_in_feats,\n",
    "                           node_out_feats=node_out_feats,\n",
    "                           edge_in_feats=edge_in_feats,\n",
    "                           edge_hidden_feats=edge_hidden_feats,\n",
    "                           num_step_message_passing=num_step_message_passing)\n",
    "        \n",
    "        self.linearB = nn.Linear(node_out_feats*2, node_out_feats)\n",
    "\n",
    "        self.att = Global_Reactivity_Attention(node_out_feats, attention_heads, attention_layers)\n",
    "        \n",
    "        self.atom_linear =  nn.Sequential(\n",
    "                            nn.Linear(node_out_feats, node_out_feats), \n",
    "                            nn.ReLU(), \n",
    "                            nn.Dropout(0.2),\n",
    "                            nn.Linear(node_out_feats, AtomTemplate_n+1))\n",
    "        self.bond_linear =  nn.Sequential(\n",
    "                            nn.Linear(node_out_feats, node_out_feats), \n",
    "                            nn.ReLU(), \n",
    "                            nn.Dropout(0.2),\n",
    "                            nn.Linear(node_out_feats, BondTemplate_n+1))\n",
    "\n",
    "    def forward(self, g, node_feats, edge_feats):\n",
    "        node_feats = self.mpnn(g, node_feats, edge_feats)\n",
    "        atom_feats = node_feats\n",
    "        bond_feats = self.linearB(pair_atom_feats(g, node_feats))\n",
    "        edit_feats, mask = unbatch_mask(g, atom_feats, bond_feats)\n",
    "        attention_score, edit_feats = self.att(edit_feats, mask)\n",
    "           \n",
    "        atom_feats, bond_feats = unbatch_feats(g, edit_feats)\n",
    "        atom_outs = self.atom_linear(atom_feats) \n",
    "        bond_outs = self.bond_linear(bond_feats) \n",
    "\n",
    "        return atom_outs, bond_outs, attention_score\n",
    "\n",
    "\n",
    "def load_model(args):\n",
    "    exp_config = get_configure(args)\n",
    "    model = LocalRetroC(\n",
    "        node_in_feats=exp_config['in_node_feats'],\n",
    "        edge_in_feats=exp_config['in_edge_feats'],\n",
    "        node_out_feats=exp_config['node_out_feats'],\n",
    "        edge_hidden_feats=exp_config['edge_hidden_feats'],\n",
    "        num_step_message_passing=exp_config['num_step_message_passing'],\n",
    "        attention_heads = exp_config['attention_heads'],\n",
    "        attention_layers = exp_config['attention_layers'],\n",
    "        AtomTemplate_n = exp_config['AtomTemplate_n'],\n",
    "        BondTemplate_n = exp_config['BondTemplate_n'])\n",
    "    model = model.to(args['device'])\n",
    "    print ('Parameters of loaded LocalRetro:')\n",
    "    print (exp_config)\n",
    "    # print(\"KKKK\")\n",
    "    # exit(0)    \n",
    "    if args['mode'] == 'train':\n",
    "        loss_criterion = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        optimizer = Adam(model.parameters(), lr=args['learning_rate'], weight_decay=args['weight_decay'])\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=args['schedule_step'])\n",
    "        \n",
    "        if os.path.exists(args['model_path']):\n",
    "            user_answer = input('%s exists, want to (a) overlap (b) continue from checkpoint (c) make a new model?' % args['model_path'])\n",
    "            if user_answer == 'a':\n",
    "                stopper = EarlyStopping(mode = 'lower', patience=args['patience'], filename=args['model_path'])\n",
    "                print ('Overlap exsited model and training a new model...')\n",
    "            elif user_answer == 'b':\n",
    "                stopper = EarlyStopping(mode = 'lower', patience=args['patience'], filename=args['model_path'])\n",
    "                stopper.load_checkpoint(model)\n",
    "                print ('Train from exsited model checkpoint...')\n",
    "            elif user_answer == 'c':\n",
    "                model_name = input('Enter new model name: ')\n",
    "                args['model_path'] = args['model_path'].replace('%s.pth' % args['dataset'], '%s.pth' % model_name)\n",
    "                stopper = EarlyStopping(mode = 'lower', patience=args['patience'], filename=args['model_path'])\n",
    "                print ('Training a new model %s.pth' % model_name)\n",
    "        else:\n",
    "            stopper = EarlyStopping(mode = 'lower', patience=args['patience'], filename=args['model_path'])\n",
    "        return model, loss_criterion, optimizer, scheduler, stopper\n",
    "    \n",
    "    else:\n",
    "        model.load_state_dict(torch.load(args['model_path'])['model_state_dict'])\n",
    "        return model\n",
    "\n",
    "def mainX(args):\n",
    "    #KKK\n",
    "    model_path = '../modelsC'\n",
    "    model_name = 'LocalRetro_%s.pth' % args['dataset']\n",
    "    args['model_path'] = model_path +'/' + model_name\n",
    "    args['config_path'] = '../data/configs/%s' % args['config']\n",
    "    args['data_dir'] = '../data/%s' % args['dataset']\n",
    "    mkdir_p(model_path)                          \n",
    "    args = init_featurizer(args)\n",
    "    model, loss_criterion, optimizer, scheduler, stopper = load_model(args)   \n",
    "    \n",
    "    return model,loss_criterion#,train_loader\n",
    "\n",
    "    # for epoch in range(args['num_epochs']):\n",
    "    #     print(\"Running\",epoch)\n",
    "    #     run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "    #     val_loss = run_an_eval_epoch(args, model, val_loader, loss_criterion)\n",
    "    #     early_stop = stopper.step(val_loss, model) \n",
    "    #     scheduler.step()\n",
    "    #     print('epoch %d/%d, validation loss: %.4f' %  (epoch + 1, args['num_epochs'], val_loss))\n",
    "    #     print('epoch %d/%d, Best loss: %.4f' % (epoch + 1, args['num_epochs'], stopper.best_score))\n",
    "    #     if early_stop:\n",
    "    #         print ('Early stopped!!')\n",
    "    #         break\n",
    "\n",
    "    # stopper.load_checkpoint(model)\n",
    "    # test_loss = run_an_eval_epoch(args, model, test_loader, loss_criterion)\n",
    "    # print('test loss: %.4f' % test_loss)\n",
    "#    return (train_loader, val_loader, test_loader)\n",
    "\n",
    "#train_loader, val_loader, test_loader = mainX(args)    \n",
    "model,loss_criterion =  mainX(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously saved dgl graphs...../data/saved_graphs/USPTO_50K_dglgraph.bin\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = load_dataloader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=230, num_edges=736,\n",
      "      ndata_schemes={'h': Scheme(shape=(80,), dtype=torch.float32)}\n",
      "      edata_schemes={'e': Scheme(shape=(13,), dtype=torch.float32)})\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, -1, 8, 1]' is invalid for input of size 11280",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\kwang\\py\\LocalRetro\\scripts\\TrainC.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kwang/py/LocalRetro/scripts/TrainC.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kwang/py/LocalRetro/scripts/TrainC.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m atom_labels, bond_labels \u001b[39m=\u001b[39m atom_labels\u001b[39m.\u001b[39mto(args[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m]), bond_labels\u001b[39m.\u001b[39mto(args[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/kwang/py/LocalRetro/scripts/TrainC.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m atom_logits, bond_logits, _ \u001b[39m=\u001b[39m predict(args, model, bg)    \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kwang/py/LocalRetro/scripts/TrainC.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(atom_logits)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kwang/py/LocalRetro/scripts/TrainC.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(bond_logits)\n",
      "File \u001b[1;32md:\\kwang\\py\\LocalRetro\\scripts\\utils.py:164\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(args, model, bg)\u001b[0m\n\u001b[0;32m    162\u001b[0m node_feats \u001b[39m=\u001b[39m bg\u001b[39m.\u001b[39mndata\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mh\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(args[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    163\u001b[0m edge_feats \u001b[39m=\u001b[39m bg\u001b[39m.\u001b[39medata\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(args[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 164\u001b[0m \u001b[39mreturn\u001b[39;00m model(bg, node_feats, edge_feats)\n",
      "File \u001b[1;32mc:\\Users\\henta\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\kwang\\py\\LocalRetro\\scripts\\TrainC.ipynb Cell 4\u001b[0m in \u001b[0;36mLocalRetroC.forward\u001b[1;34m(self, g, node_feats, edge_feats)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kwang/py/LocalRetro/scripts/TrainC.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m bond_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinearB(pair_atom_feats(g, node_feats))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kwang/py/LocalRetro/scripts/TrainC.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m edit_feats, mask \u001b[39m=\u001b[39m unbatch_mask(g, atom_feats, bond_feats)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/kwang/py/LocalRetro/scripts/TrainC.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m attention_score, edit_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt(edit_feats, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kwang/py/LocalRetro/scripts/TrainC.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m atom_feats, bond_feats \u001b[39m=\u001b[39m unbatch_feats(g, edit_feats)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kwang/py/LocalRetro/scripts/TrainC.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m atom_outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matom_linear(atom_feats) \n",
      "File \u001b[1;32mc:\\Users\\henta\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\kwang\\py\\LocalRetro\\scripts\\model_utils.py:122\u001b[0m, in \u001b[0;36mGlobal_Reactivity_Attention.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    120\u001b[0m scores \u001b[39m=\u001b[39m []\n\u001b[0;32m    121\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_layers):\n\u001b[1;32m--> 122\u001b[0m     score, x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt_stack[n](x, mask)\n\u001b[0;32m    123\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpff_stack[n](x)\n\u001b[0;32m    124\u001b[0m     scores\u001b[39m.\u001b[39mappend(score)\n",
      "File \u001b[1;32mc:\\Users\\henta\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\kwang\\py\\LocalRetro\\scripts\\model_utils.py:80\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     79\u001b[0m     bs \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_linear(x)\u001b[39m.\u001b[39;49mview(bs, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mh, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md_k)\n\u001b[0;32m     81\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_linear(x)\u001b[39m.\u001b[39mview(bs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_k)\n\u001b[0;32m     82\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_linear(x)\u001b[39m.\u001b[39mview(bs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_k)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[8, -1, 8, 1]' is invalid for input of size 11280"
     ]
    }
   ],
   "source": [
    "# print(model_path)\n",
    "for batch_id, batch_data in enumerate(train_loader):\n",
    "    smiles, bg, atom_labels, bond_labels = batch_data\n",
    "    # print(smiles,len(smiles))\n",
    "    print(bg)\n",
    "    # print(atom_labels,len(atom_labels))\n",
    "    # print(bond_labels,len(bond_labels))\n",
    "\n",
    "    if len(smiles) == 1:\n",
    "        continue\n",
    "           \n",
    "    atom_labels, bond_labels = atom_labels.to(args['device']), bond_labels.to(args['device'])\n",
    "    atom_logits, bond_logits, _ = predict(args, model, bg)    \n",
    "    print(atom_logits)\n",
    "    print(bond_logits)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "g1 = dgl.graph(([0, 1], [1, 0]))\n",
    "g1.ndata['h'] = torch.tensor([1., 2.])\n",
    "g2 = dgl.graph(([0, 1], [1, 2]))\n",
    "g2.ndata['h'] = torch.tensor([1., 2., 3.])\n",
    "\n",
    "dgl.readout_nodes(g1, 'h')\n",
    "# tensor([3.])  # 1 + 2\n",
    "\n",
    "bg = dgl.batch([g1, g2])\n",
    "dgl.readout_nodes(bg, 'h')\n",
    "# tensor([3., 6.])  # [1 + 2, 1 + 2 + 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumTrainLoader = enumerate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id, batch_data = next(enumTrainLoader)\n",
    "smiles, bg, atom_labels, bond_labels = batch_data\n",
    "atom_labels, bond_labels = atom_labels.to(args['device']), bond_labels.to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CH2:1]([CH2:2][CH2:3][c:4]1[cH:5][cH:6][cH:7][cH:8][cH:9]1)[NH:10][CH2:11][CH2:12][CH2:13][CH2:14][CH2:15][n:16]1[cH:17][cH:18][n:19][cH:20]1'],\n",
       " Graph(num_nodes=20, num_edges=62,\n",
       "       ndata_schemes={'h': Scheme(shape=(80,), dtype=torch.float32)}\n",
       "       edata_schemes={'e': Scheme(shape=(13,), dtype=torch.float32)}),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0, 545,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id, batch_data = next(enumTrainLoader)\n",
    "smiles, bg, atom_labels, bond_labels = batch_data\n",
    "atom_labels, bond_labels = atom_labels.to(args['device']), bond_labels.to(args['device'])\n",
    "bg = bg.to(args['device'])\n",
    "node_feats = bg.ndata.pop('h').to(args['device'])\n",
    "edge_feats = bg.edata.pop('e').to(args['device'])\n",
    "    # return model(bg, node_feats, edge_feats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 80]) torch.Size([53, 13])\n",
      "pair_atom_feats tensor([[ 0.7240, -0.8444,  0.5521,  ..., -0.2495, -0.9935,  0.1580],\n",
      "        [ 0.7240, -0.8444,  0.5521,  ..., -0.1911, -0.9167,  0.6131],\n",
      "        [-0.1346, -0.9412,  0.5618,  ..., -0.0635, -0.9048,  0.6645],\n",
      "        ...,\n",
      "        [-0.2897, -0.9593,  0.5600,  ..., -0.1647, -0.9951, -0.1582],\n",
      "        [-0.2592, -0.9607,  0.5665,  ..., -0.2900, -0.9978,  0.3605],\n",
      "        [-0.2592, -0.9607,  0.5665,  ..., -0.1593, -0.9939, -0.0664]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(node_feats.size(),edge_feats.size())\n",
    "atom_logits, bond_logits, _= model(bg, node_feats, edge_feats)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 125]) torch.Size([42, 549])\n"
     ]
    }
   ],
   "source": [
    "print(atom_logits.size(),bond_logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 125]) torch.Size([36, 549])\n",
      "torch.Size([17]) torch.Size([36])\n"
     ]
    }
   ],
   "source": [
    "print(atom_logits.size(),bond_logits.size())\n",
    "print(atom_labels.size(),bond_labels.size())\n",
    "loss_a = loss_criterion(atom_logits, atom_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 125]) torch.Size([36, 549])\n"
     ]
    }
   ],
   "source": [
    "_atom_logits = nn.Softmax(dim = 1)(atom_logits)\n",
    "_bond_logits = nn.Softmax(dim = 1)(bond_logits) \n",
    "print(_atom_logits.size(),_bond_logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "def get_bg_partition(bg):\n",
    "    sg = bg.remove_self_loop()\n",
    "    gs = dgl.unbatch(sg)\n",
    "    nodes_sep = [0]\n",
    "    edges_sep = [0]\n",
    "    for g in gs:\n",
    "        nodes_sep.append(nodes_sep[-1] + g.num_nodes())\n",
    "        edges_sep.append(edges_sep[-1] + g.num_edges())\n",
    "    return gs, nodes_sep[1:], edges_sep[1:]\n",
    "\n",
    "graphs, nodes_sep, edges_sep = get_bg_partition(bg)    \n",
    "genum = enumerate(zip(graphs, nodes_sep, edges_sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] [36]\n",
      "[Graph(num_nodes=17, num_edges=36,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " (Graph(num_nodes=17, num_edges=36,\n",
       "        ndata_schemes={}\n",
       "        edata_schemes={}),\n",
       "  17,\n",
       "  36))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nodes_sep,edges_sep)\n",
    "print(graphs)\n",
    "next(genum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a', 'a', 'a', 'a'] [(9, 13), (8, 13), (5, 13), (1, 13), (0, 13)] [0.017958768, 0.016977806, 0.016885, 0.016703364, 0.016456636]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def combined_edit(graph, atom_out, bond_out, top_num):\n",
    "    edit_id_a, edit_proba_a = output2edit(atom_out, top_num)\n",
    "    edit_id_b, edit_proba_b = output2edit(bond_out, top_num)\n",
    "    edit_id_c = edit_id_a + edit_id_b\n",
    "    edit_type_c = ['a'] * top_num + ['b'] * top_num\n",
    "    edit_proba_c = edit_proba_a + edit_proba_b\n",
    "    edit_rank_c = np.flip(np.argsort(edit_proba_c))[:top_num]\n",
    "    edit_type_c = [edit_type_c[r] for r in edit_rank_c]\n",
    "    edit_id_c = [edit_id_c[r] for r in edit_rank_c]\n",
    "    edit_proba_c = [edit_proba_c[r] for r in edit_rank_c]\n",
    "    return edit_type_c, edit_id_c, edit_proba_c\n",
    "\n",
    "def output2edit(out, top_num):\n",
    "    class_n = out.size(-1)\n",
    "    readout = out.cpu().detach().numpy()\n",
    "    readout = readout.reshape(-1)\n",
    "    output_rank = np.flip(np.argsort(readout))\n",
    "    output_rank = [r for r in output_rank if get_id_template(r, class_n)[1] != 0][:top_num]\n",
    "    selected_edit = [get_id_template(a, class_n) for a in output_rank]\n",
    "    selected_proba = [readout[a] for a in output_rank]\n",
    "    return selected_edit, selected_proba\n",
    "\n",
    "def get_id_template(a, class_n):\n",
    "    class_n = class_n # no template\n",
    "    edit_idx = a//class_n\n",
    "    template = a%class_n\n",
    "    return (edit_idx, template)\n",
    "\n",
    "start_node=0\n",
    "start_edge=0\n",
    "args['top_num'] = 5\n",
    "for single_id, (graph, end_node, end_edge) in enumerate(zip(graphs, nodes_sep, edges_sep)):\n",
    "    # smiles = smiles_list[single_id]\n",
    "    test_id = (batch_id * args['batch_size']) + single_id\n",
    "    pred_types, pred_sites, pred_scores = combined_edit(graph, _atom_logits[start_node:end_node], _bond_logits[start_edge:end_edge], args['top_num'])\n",
    "    start_node = end_node\n",
    "    start_edge = end_edge\n",
    "    print(pred_types,pred_sites,pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0074, 0.0094, 0.0100, 0.0101, 0.0094, 0.0136, 0.0089, 0.0120, 0.0088,\n",
       "        0.0061, 0.0109, 0.0092, 0.0076, 0.0165, 0.0072, 0.0064, 0.0069, 0.0070,\n",
       "        0.0086, 0.0086, 0.0105, 0.0061, 0.0077, 0.0060, 0.0093, 0.0060, 0.0073,\n",
       "        0.0086, 0.0053, 0.0092, 0.0107, 0.0083, 0.0074, 0.0056, 0.0069, 0.0066,\n",
       "        0.0089, 0.0137, 0.0070, 0.0063, 0.0088, 0.0084, 0.0079, 0.0085, 0.0087,\n",
       "        0.0096, 0.0100, 0.0055, 0.0093, 0.0060, 0.0081, 0.0065, 0.0106, 0.0054,\n",
       "        0.0056, 0.0096, 0.0052, 0.0061, 0.0073, 0.0075, 0.0098, 0.0067, 0.0046,\n",
       "        0.0093, 0.0054, 0.0036, 0.0105, 0.0080, 0.0059, 0.0062, 0.0067, 0.0058,\n",
       "        0.0087, 0.0121, 0.0078, 0.0087, 0.0048, 0.0087, 0.0090, 0.0071, 0.0074,\n",
       "        0.0067, 0.0063, 0.0076, 0.0083, 0.0058, 0.0069, 0.0053, 0.0040, 0.0060,\n",
       "        0.0039, 0.0082, 0.0076, 0.0050, 0.0067, 0.0096, 0.0092, 0.0101, 0.0082,\n",
       "        0.0092, 0.0082, 0.0057, 0.0103, 0.0109, 0.0100, 0.0070, 0.0091, 0.0113,\n",
       "        0.0061, 0.0087, 0.0086, 0.0095, 0.0067, 0.0071, 0.0124, 0.0066, 0.0092,\n",
       "        0.0079, 0.0082, 0.0072, 0.0086, 0.0102, 0.0075, 0.0081, 0.0071],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_atom_logits[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efb793b6751021ca5c941ae7896bf1b4ef1dcec9122e0a80c298c50a9e96a60a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
